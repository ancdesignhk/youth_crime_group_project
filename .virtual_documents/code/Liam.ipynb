


# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time

# from sklearn
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error


#youth_crime = pd.read_csv('../data/state_demo_crime_youth_data_combined_clean.csv')
#youth_crime =pd.read_csv('../data/data.csv')
youth_crime =pd.read_csv('../data/data_engineered.csv')
youth_crime.head()


# Define features (X) and target variable (y)

# columns to exclude: population-related and crime-related features
crime_related_columns = [
    'crimes_against_society',
    'fraud_and_other_financial_crimes',
    'property_crime',
    'violent_crime',
    'total_crime_count',
    'log_total_crime_count'
]

population_related_columns = [
    'total_pop',
    'white_pop',
    'black_pop',
    'hispanic_pop',
    'asian_pop',
    'native_pop',
    'islander_pop',
    'multi_race_pop',
    'state'
]



#features to keep: exclude population-related and crime-related features
X = youth_crime.drop(columns=crime_related_columns + population_related_columns)

# Display the list of features used for modeling
filtered_features = X.columns.tolist()
print(filtered_features)


y = youth_crime['log_total_crime_count']




# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f'X_train shape: {X_train.shape}')
print(f'X_test shape: {X_test.shape}')
print(f'y_train shape: {y_train.shape}')
print(f'y_test shape: {y_test.shape}')


'''
# Preprocessing: Encode year, one-hot encode state, and standardize numeric features
columns_trans = ColumnTransformer(
    transformers=[
        ('onehot', OneHotEncoder(drop='first',sparse_output=False,handle_unknown='ignore'), ['state']),
        ('ordinal', OrdinalEncoder(), ['year']),
        ('num', StandardScaler(),[ 'median_income', 'poverty_rate', 'unemployment_rate', 'unemployed_15_weeks', 'labor_force_participation_rate', 'hs_grad_rate', 'bachelors_grad_rate', 'zhvi', 'crude_rate_suicide', 'crude_rate_od', 'youth_not_in_school', 'youth_in_foster_care', 'youth_living_in_poverty'])
    ],
 remainder='passthrough'
)'''








# create features matrix
#X = youth_crime[numerical_features + ordinal_features + nominal_features]
'''
# create target variable, log target variable
#y = youth_crime['log_total_youth_crime']

# process X
X_process = columns_trans.fit_transform(X)

# save features
X_process_features = columns_trans.fit(X).get_feature_names_out()

# train-test split
X_train, X_test, y_train, y_test = train_test_split(X_process, y,
                                                    random_state = 42,
                                                    test_size = 0.2)

print(f'X_train shape: {X_train.shape}')
print(f'X_test shape: {X_test.shape}')
print(f'y_train shape: {y_train.shape}')
print(f'y_test shape: {y_test.shape}')'''








# establish Logistic Regression pipeline with CountVectorizer
pipe_dt = Pipeline([
    ('dt', DecisionTreeRegressor(random_state = 123,
                                 max_depth = 5,
                                 min_samples_split = 7,
                                 min_samples_leaf = 3,
                                 ccp_alpha = 0.01))
])
# calculate cross validation score mean
print(f'Cross Validation mean: {cross_val_score(pipe_dt, X_train, y_train, cv = 3).mean()}')

# fit model to training data
pipe_dt.fit(X_train, y_train)
# Make predictions
y_train_pred = pipe_dt.predict(X_train)
y_test_pred = pipe_dt.predict(X_test)
#Evaluate Model
print(" Decision Tree Model")
# training 
print(f'Training MSE: {mean_squared_error(y_train,y_train_pred)}')
print(f'Training MAE: {mean_absolute_error(y_train,y_train_pred)}')
print(f'Training R2 {r2_score(y_train,y_train_pred)}')

# test 
print(f'Testing MSE: {mean_squared_error(y_test,y_test_pred)}')
print(f'Testing MAE: {mean_absolute_error( y_test,y_test_pred)}')
print(f'Testing R2: {r2_score(y_test,y_test_pred)}')





grid_dt = GridSearchCV(estimator = DecisionTreeRegressor(random_state = 123),
                    param_grid = {'max_depth': range(2,8,1),
                                  'min_samples_split': range(8,25,3),
                                  'min_samples_leaf': range(2,7),
                                  'ccp_alpha': [0, 0.001, 0.01, 0.1, 1, 10]},
                    cv = 5,
                    verbose = 1)


# start timer
t0 = time.time()

# print start time
print(t0)

# gridsearch
grid_dt.fit(X_train, y_train)

# print end time
print(time.time() - t0)


best_model_dt = grid_dt.best_estimator_

best_model_dt


grid_dt.best_score_


# Make predictions
y_train_pred = grid_dt.predict(X_train)
y_test_pred = grid_dt.predict(X_test)
#Evaluate Model
print(" Grid Search Decision Tree Model")
# training 
print(f'Training MSE: {mean_squared_error(y_train,y_train_pred)}')
print(f'Training MAE: {mean_absolute_error(y_train,y_train_pred)}')
print(f'Training R2 {r2_score(y_train,y_train_pred)}')

# test 
print(f'Testing MSE: {mean_squared_error(y_test,y_test_pred)}')
print(f'Testing MAE: {mean_absolute_error( y_test,y_test_pred)}')
print(f'Testing R2: {r2_score(y_test,y_test_pred)}')


feat_importances = pd.Series(best_model_dt.feature_importances_, index = X_train.columns)
feat_importances.nlargest(15).plot(kind = 'barh')
plt.title('Top 15 Features - Decision Tree')

plt.show()





# 
pipe_rf = Pipeline([
    ('rf', RandomForestRegressor(random_state = 42,
                                 max_depth = 5,
                                 min_samples_split = 7,
                                 min_samples_leaf = 3,
                                 ccp_alpha = 0.01))
])


# calculate cross validation score mean
print(f'Cross Validation mean: {cross_val_score(pipe_rf, X_train, y_train, cv = 3).mean()}')

# fit model to training data
pipe_rf.fit(X_train, y_train)

# Make predictions
y_train_pred = pipe_rf.predict(X_train)
y_test_pred = pipe_rf.predict(X_test)
#Evaluate Model
print(" Random Forest Model")
# training 
print(f'Training MSE: {mean_squared_error(y_train,y_train_pred)}')
print(f'Training MAE: {mean_absolute_error(y_train,y_train_pred)}')
print(f'Training R2 {r2_score(y_train,y_train_pred)}')

# test 
print(f'Testing MSE: {mean_squared_error(y_test,y_test_pred)}')
print(f'Testing MAE: {mean_absolute_error( y_test,y_test_pred)}')
print(f'Testing R2: {r2_score(y_test,y_test_pred)}')





grid_rf = GridSearchCV(estimator = RandomForestRegressor(random_state = 42, n_jobs = -1),
                    param_grid = {'max_depth': range(2,8,1),
                                  'min_samples_split': range(8,25,3),
                                  'min_samples_leaf': range(2,7),
                                  'ccp_alpha': [0, 0.001, 0.01, 0.1, 1, 10]},
                    cv = 5,
                    verbose = 1)


# start timer
t0 = time.time()

# print start time
print(t0)

# gridsearch
grid_rf.fit(X_train, y_train)

# print end time
print(time.time() - t0)


best_model_rf = grid_rf.best_estimator_

best_model_rf


grid_rf.best_score_


# Make predictions
y_train_pred = best_model_rf.predict(X_train)
y_test_pred = best_model_rf.predict(X_test)
#Evaluate Model
print(" Random Forest Model")
# training 
print(f'Training MSE: {mean_squared_error(y_train,y_train_pred)}')
print(f'Training MAE: {mean_absolute_error(y_train,y_train_pred)}')
print(f'Training R2 {r2_score(y_train,y_train_pred)}')

# test 
print(f'Testing MSE: {mean_squared_error(y_test,y_test_pred)}')
print(f'Testing MAE: {mean_absolute_error( y_test,y_test_pred)}')
print(f'Testing R2: {r2_score(y_test,y_test_pred)}')


print(f'Score on training set: {grid_rf.score(X_train, y_train)}')
print(f'Score on testing set: {grid_rf.score(X_test, y_test)}')


feat_importances = pd.Series(best_model_rf.feature_importances_, index = X_train.columns)
feat_importances.nlargest(15).plot(kind = 'barh')
plt.title('Top 15 Features - Random Forest')




