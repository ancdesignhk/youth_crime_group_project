


# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time

# from sklearn
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor


youth_crime = pd.read_csv('../data/state_demo_crime_youth_data_combined_clean.csv')


# drop other youth crime columns after creating combined youth crime target variable
print(youth_crime.shape)
youth_crime.drop(columns = ['crimes_against_society',
                            'fraud_and_other_financial_crimes',
                            'property_crime',
                            'violent_crime'],
                inplace = True)
print(youth_crime.shape)


# create numerical features list
numerical_features = list(youth_crime.drop(columns = ['state','year','total_youth_crime']).columns)

numerical_features


# create ordinal features list
ordinal_features = ['year']

ordinal_features


# create nominal features list
nominal_features = ['state']

nominal_features





# create numerical features pipeline
num_pipe = Pipeline([
    ('si', SimpleImputer(strategy = 'median')), # imputing median values for suppressed youth suicide and OD rates
    ('ss', StandardScaler()) # scaling features
])


# create ordinal features pipeline
ord_pipe = Pipeline([
    ('ohe', OrdinalEncoder()) # ordinal encoding for ordinal variables
])


# create nominal features pipeline
nom_pipe = Pipeline([
    ('ohe', OneHotEncoder(drop = 'first')) # one-hot encoding for nominal variables
])


# combine pipelines
columns_trans = ColumnTransformer([
    ('num_processor', num_pipe, numerical_features),
    ('ord_processor', ord_pipe, ordinal_features),
    ('nom_processor', nom_pipe, nominal_features)
])





# create features matrix
X = youth_crime[numerical_features + ordinal_features + nominal_features]

# create target variable, log target variable
y = youth_crime['total_youth_crime']

# process X
X_process = columns_trans.fit_transform(X)

# save features
X_process_features = columns_trans.fit(X).get_feature_names_out()

# train-test split
X_train, X_test, y_train, y_test = train_test_split(X_process, y,
                                                    random_state = 123,
                                                    test_size = 0.25)

print(f'X_train shape: {X_train.shape}')
print(f'X_test shape: {X_test.shape}')
print(f'y_train shape: {y_train.shape}')
print(f'y_test shape: {y_test.shape}')








# establish Logistic Regression pipeline with CountVectorizer
pipe_dt = Pipeline([
    ('dt', DecisionTreeRegressor(random_state = 123,
                                 max_depth = 5,
                                 min_samples_split = 7,
                                 min_samples_leaf = 3,
                                 ccp_alpha = 0.01))
])


# calculate cross validation score mean
print(f'Cross Validation mean: {cross_val_score(pipe_dt, X_train, y_train, cv = 3).mean()}')

# fit model to training data
pipe_dt.fit(X_train, y_train)

# training accuracy
print(f'Training accuracy: {pipe_dt.score(X_train, y_train)}')

# test accuracy
print(f'Testing accuracy: {pipe_dt.score(X_test, y_test)}')





grid_dt = GridSearchCV(estimator = DecisionTreeRegressor(random_state = 123),
                    param_grid = {'max_depth': range(2,8,1),
                                  'min_samples_split': range(8,25,3),
                                  'min_samples_leaf': range(2,7),
                                  'ccp_alpha': [0, 0.001, 0.01, 0.1, 1, 10]},
                    cv = 5,
                    verbose = 1)


# start timer
t0 = time.time()

# print start time
print(t0)

# gridsearch
grid_dt.fit(X_train, y_train)

# print end time
print(time.time() - t0)


best_model_dt = grid_dt.best_estimator_

best_model_dt


grid_dt.best_score_


print(f'Score on training set: {grid_dt.score(X_train, y_train)}')
print(f'Score on testing set: {grid_dt.score(X_test, y_test)}')


feat_importances = pd.Series(best_model_dt.feature_importances_, index = X_process_features)
feat_importances.nlargest(15).plot(kind = 'barh')
plt.title('Top 15 Features - Decision Tree')
plt.show()





# establish Logistic Regression pipeline with CountVectorizer
pipe_rf = Pipeline([
    ('rf', RandomForestRegressor(random_state = 123,
                                 max_depth = 5,
                                 min_samples_split = 7,
                                 min_samples_leaf = 3,
                                 ccp_alpha = 0.01))
])


# calculate cross validation score mean
print(f'Cross Validation mean: {cross_val_score(pipe_rf, X_train, y_train, cv = 3).mean()}')

# fit model to training data
pipe_rf.fit(X_train, y_train)

# training accuracy
print(f'Training accuracy: {pipe_rf.score(X_train, y_train)}')

# test accuracy
print(f'Testing accuracy: {pipe_rf.score(X_test, y_test)}')





grid_rf = GridSearchCV(estimator = RandomForestRegressor(random_state = 123, n_jobs = -1),
                    param_grid = {'max_depth': range(2,8,1),
                                  'min_samples_split': range(8,25,3),
                                  'min_samples_leaf': range(2,7),
                                  'ccp_alpha': [0, 0.001, 0.01, 0.1, 1, 10]},
                    cv = 5,
                    verbose = 1)


# start timer
t0 = time.time()

# print start time
print(t0)

# gridsearch
grid_rf.fit(X_train, y_train)

# print end time
print(time.time() - t0)


best_model_rf = grid_rf.best_estimator_

best_model_rf


grid_rf.best_score_


print(f'Score on training set: {grid_rf.score(X_train, y_train)}')
print(f'Score on testing set: {grid_rf.score(X_test, y_test)}')


feat_importances = pd.Series(best_model_rf.feature_importances_, index = X_process_features)
feat_importances.nlargest(15).plot(kind = 'barh')
plt.title('Top 15 Features - Random Forest')
plt.show();
